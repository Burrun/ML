#!/usr/bin/env python3
"""
CSV íŒŒì¼ì—ì„œ insn_addrì´ ë¹„ì–´ìˆëŠ” íŒŒì¼(timeoutëœ íŒŒì¼)ì„ ì œê±°í•˜ëŠ” ìŠ¤í¬ë¦½íŠ¸

ì‚¬ìš©ë²•:
    # í•™ìŠµ ë°ì´í„°ì…‹ í•„í„°ë§
    python3 src/filter_timeout_files.py --csv data/train.csv --csv data/valid.csv --csv data/test.csv

    # TEST í´ë” ë‚´ íŠ¹ì • ê³µê²©ê¸°ë²• í•„í„°ë§
    python3 src/filter_timeout_files.py --csv TEST/ExtendDOS/test.csv
    python3 src/filter_timeout_files.py --csv TEST/Header/test.csv
    python3 src/filter_timeout_files.py --csv TEST/Kreuk/test.csv
    python3 src/filter_timeout_files.py --csv TEST/Padding/test.csv
    python3 src/filter_timeout_files.py --csv TEST/Slack/test.csv

    # TEST í´ë” ë‚´ ëª¨ë“  ê³µê²©ê¸°ë²• í•œë²ˆì— í•„í„°ë§
    python3 src/filter_timeout_files.py --csv TEST/ExtendDOS/test.csv --csv TEST/Header/test.csv --csv TEST/Kreuk/test.csv --csv TEST/Padding/test.csv --csv TEST/Slack/test.csv
"""

import argparse
import csv
import os
import torch
import shutil
import random
from pathlib import Path
from typing import List, Dict

def check_metadata_valid(metadata_path: str, root_dir: str = None) -> bool:
    """ë©”íƒ€ë°ì´í„° íŒŒì¼ì´ ìœ íš¨í•œì§€ í™•ì¸ (insn_addrì´ ë¹„ì–´ìˆì§€ ì•Šì€ì§€)"""
    full_path = metadata_path
    if root_dir:
        full_path = os.path.join(root_dir, metadata_path)
    
    if not os.path.exists(full_path):
        # print(f"  âš ï¸  ë©”íƒ€ë°ì´í„° íŒŒì¼ ì—†ìŒ: {full_path}")
        return False
    
    try:
        metadata = torch.load(full_path)
        
        # insn_addr ì²´í¬
        if 'insn_addr' not in metadata:
            # print(f"  âš ï¸  insn_addr í‚¤ ì—†ìŒ: {full_path}")
            return False
        
        insn_addr = metadata['insn_addr']
        # sparse tensorì˜ ê²½ìš° _nnz()ë¡œ 0ì´ ì•„ë‹Œ ìš”ì†Œ ê°œìˆ˜ í™•ì¸
        if hasattr(insn_addr, '_nnz'):
            if insn_addr._nnz() == 0:
                # print(f"  âŒ insn_addr ë¹„ì–´ìˆìŒ (timeout): {full_path}")
                return False
        elif insn_addr.sum() == 0:
            # print(f"  âŒ insn_addr ë¹„ì–´ìˆìŒ (timeout): {full_path}")
            return False
        
        return True
    
    except Exception as e:
        print(f"  âš ï¸  ë©”íƒ€ë°ì´í„° ë¡œë“œ ì‹¤íŒ¨: {full_path} - {e}")
        return False

def move_file(src: str, dest: str):
    """íŒŒì¼ ì´ë™ (ë””ë ‰í† ë¦¬ ìƒì„± í¬í•¨)"""
    os.makedirs(os.path.dirname(dest), exist_ok=True)
    shutil.move(src, dest)

def filter_timeout_files(csv_paths: List[str], root_dir: str = None):
    """CSVì—ì„œ timeoutëœ íŒŒì¼ ì œê±° ë° ìƒˆ CSV ì €ì¥"""
    
    print(f"\n{'='*80}")
    print("ğŸ” CSV í•„í„°ë§ ì‹œì‘ (Timeout íŒŒì¼ ì œê±°)")
    print(f"{'='*80}")
    
    for csv_path in csv_paths:
        if not os.path.exists(csv_path):
            print(f"âš ï¸  CSV íŒŒì¼ ì—†ìŒ: {csv_path}")
            continue
        
        # CSV ë°±ì—… ìƒì„±
        backup_path = csv_path + ".backup"
        shutil.copy2(csv_path, backup_path)
        print(f"ğŸ’¾ ë°±ì—… ìƒì„±: {csv_path} â†’ {backup_path}")
        
        # CSV ì½ê¸°
        rows = []
        with open(csv_path, 'r') as f:
            reader = csv.DictReader(f)
            for row in reader:
                rows.append(row)
        
        total_count = len(rows)
        print(f"ğŸ“– ì½ì€ ë°ì´í„°: {total_count}ê°œ")
        
        # ìœ íš¨ì„± ê²€ì‚¬
        valid_rows = []
        
        # root_dir ìë™ ê°ì§€ (ì§€ì •ë˜ì§€ ì•Šì€ ê²½ìš°)
        current_root_dir = root_dir
        if current_root_dir is None and len(rows) > 0:
            # ì²« ë²ˆì§¸ í•­ëª©ìœ¼ë¡œ í…ŒìŠ¤íŠ¸
            first_meta = rows[0]['metadata_path']
            if not os.path.exists(first_meta):
                # CSV íŒŒì¼ì´ ìˆëŠ” ë””ë ‰í† ë¦¬ë¥¼ rootë¡œ ì‹œë„
                csv_dir = os.path.dirname(csv_path)
                if os.path.exists(os.path.join(csv_dir, first_meta)):
                    current_root_dir = csv_dir
                    print(f"â„¹ï¸  Root directory ìë™ ê°ì§€: {current_root_dir}")

        for row in rows:
            if check_metadata_valid(row['metadata_path'], current_root_dir):
                valid_rows.append(row)
        
        removed_count = total_count - len(valid_rows)
        print(f"âœ… ìœ íš¨í•œ ë°ì´í„°: {len(valid_rows)}ê°œ")
        print(f"âŒ ì œê±°ëœ ë°ì´í„° (Timeout): {removed_count}ê°œ")
        
        # ìƒˆ CSV ì €ì¥
        with open(csv_path, 'w', newline='') as f:
            writer = csv.DictWriter(f, fieldnames=['path', 'metadata_path', 'target', 'class'])
            writer.writeheader()
            writer.writerows(valid_rows)
        
        print(f"ğŸ’¾ ì €ì¥ ì™„ë£Œ: {csv_path}")
        print()
    
    print(f"{'='*80}")
    print("âœ… ëª¨ë“  CSV í•„í„°ë§ ì™„ë£Œ!")
    print(f"{'='*80}")

if __name__ == "__main__":
    parser = argparse.ArgumentParser(
        description="CSVì—ì„œ insn_addrì´ ë¹„ì–´ìˆëŠ” íŒŒì¼ ì œê±°"
    )
    parser.add_argument(
        "--csv",
        type=str,
        action='append',
        required=True,
        help="ì…ë ¥ CSV íŒŒì¼ ê²½ë¡œ (ì—¬ëŸ¬ ê°œ ì§€ì • ê°€ëŠ¥)"
    )
    
    parser.add_argument(
        "--root-dir",
        type=str,
        default=None,
        help="ë©”íƒ€ë°ì´í„° íŒŒì¼ì˜ ë£¨íŠ¸ ë””ë ‰í† ë¦¬ (ì„ íƒì‚¬í•­)"
    )
    
    args = parser.parse_args()
    
    filter_timeout_files(args.csv, args.root_dir)
