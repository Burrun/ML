#!/usr/bin/env python3
"""
CSV íŒŒì¼ì—ì„œ insn_addrì´ ë¹„ì–´ìˆëŠ” íŒŒì¼(timeoutëœ íŒŒì¼)ì„ ì œê±°í•˜ê³ ,
ë°ì´í„°ì…‹ì„ 6:2:2 ë¹„ìœ¨ë¡œ ì¬ë¶„í• í•˜ì—¬ íŒŒì¼ì„ ì´ë™ì‹œí‚¤ëŠ” ìŠ¤í¬ë¦½íŠ¸

ì‚¬ìš©ë²•:
    python3 src/filter_timeout_files.py --csv data/train.csv --csv data/valid.csv --csv data/test.csv --reorganize
"""

import argparse
import csv
import os
import torch
import shutil
import random
from pathlib import Path
from typing import List, Dict

def check_metadata_valid(metadata_path: str, root_dir: str = None) -> bool:
    """ë©”íƒ€ë°ì´í„° íŒŒì¼ì´ ìœ íš¨í•œì§€ í™•ì¸ (insn_addrì´ ë¹„ì–´ìˆì§€ ì•Šì€ì§€)"""
    if root_dir:
        metadata_path = os.path.join(root_dir, metadata_path)
    
    if not os.path.exists(metadata_path):
        # print(f"  âš ï¸  ë©”íƒ€ë°ì´í„° íŒŒì¼ ì—†ìŒ: {metadata_path}")
        return False
    
    try:
        metadata = torch.load(metadata_path)
        
        # insn_addr ì²´í¬
        if 'insn_addr' not in metadata:
            # print(f"  âš ï¸  insn_addr í‚¤ ì—†ìŒ: {metadata_path}")
            return False
        
        insn_addr = metadata['insn_addr']
        # sparse tensorì˜ ê²½ìš° _nnz()ë¡œ 0ì´ ì•„ë‹Œ ìš”ì†Œ ê°œìˆ˜ í™•ì¸
        if hasattr(insn_addr, '_nnz'):
            if insn_addr._nnz() == 0:
                # print(f"  âŒ insn_addr ë¹„ì–´ìˆìŒ (timeout): {metadata_path}")
                return False
        elif insn_addr.sum() == 0:
            # print(f"  âŒ insn_addr ë¹„ì–´ìˆìŒ (timeout): {metadata_path}")
            return False
        
        return True
    
    except Exception as e:
        print(f"  âš ï¸  ë©”íƒ€ë°ì´í„° ë¡œë“œ ì‹¤íŒ¨: {metadata_path} - {e}")
        return False

def move_file(src: str, dest: str):
    """íŒŒì¼ ì´ë™ (ë””ë ‰í† ë¦¬ ìƒì„± í¬í•¨)"""
    os.makedirs(os.path.dirname(dest), exist_ok=True)
    shutil.move(src, dest)

def reorganize_dataset(csv_paths: List[str], root_dir: str = None):
    """ë°ì´í„°ì…‹ ì¬êµ¬ì„±: í•„í„°ë§ -> ì…”í”Œ -> ë¶„í•  -> ì´ë™"""
    
    print(f"\n{'='*80}")
    print("ğŸ”„ ë°ì´í„°ì…‹ ì¬êµ¬ì„± ì‹œì‘ (í•„í„°ë§ + ì…”í”Œ + ë¶„í•  + ì´ë™)")
    print(f"{'='*80}")

    all_rows = []
    
    # 1. ëª¨ë“  CSV ì½ê¸°
    print("ğŸ“– CSV íŒŒì¼ ì½ëŠ” ì¤‘...")
    for csv_path in csv_paths:
        if not os.path.exists(csv_path):
            continue
        with open(csv_path, 'r') as f:
            reader = csv.DictReader(f)
            for row in reader:
                all_rows.append(row)
    
    print(f"  ì´ ì½ì€ ë°ì´í„°: {len(all_rows)}ê°œ")

    # 2. ìœ íš¨ì„± ê²€ì‚¬ (í•„í„°ë§)
    print("ğŸ” ìœ íš¨ì„± ê²€ì‚¬ ì¤‘ (Timeout íŒŒì¼ ì œê±°)...")
    valid_rows = []
    for row in all_rows:
        if check_metadata_valid(row['metadata_path'], root_dir):
            valid_rows.append(row)
    
    print(f"  ìœ íš¨í•œ ë°ì´í„°: {len(valid_rows)}ê°œ (ì œê±°ë¨: {len(all_rows) - len(valid_rows)}ê°œ)")

    # 3. ì…”í”Œ
    print("ğŸ”€ ë°ì´í„° ì…”í”Œ ì¤‘...")
    random.seed(42) # ì¬í˜„ì„±ì„ ìœ„í•´ ì‹œë“œ ê³ ì •
    random.shuffle(valid_rows)

    # 4. ë¶„í•  (6:2:2)
    total = len(valid_rows)
    n_train = int(total * 0.6)
    n_valid = int(total * 0.2)
    n_test = total - n_train - n_valid

    splits = {
        'train': valid_rows[:n_train],
        'valid': valid_rows[n_train:n_train+n_valid],
        'test': valid_rows[n_train+n_valid:]
    }

    print(f"ğŸ“Š ë¶„í•  ê²°ê³¼: Train({len(splits['train'])}) / Valid({len(splits['valid'])}) / Test({len(splits['test'])})")

    # 5. íŒŒì¼ ì´ë™ ë° CSV ì €ì¥
    print("ğŸšš íŒŒì¼ ì´ë™ ë° CSV ì €ì¥ ì¤‘...")
    
    # ê¸°ë³¸ ê²½ë¡œ ì„¤ì • (data/binary, data/metadata)
    base_binary_dir = "data/binary"
    base_metadata_dir = "data/metadata"

    for split_name, rows in splits.items():
        new_csv_path = f"data/{split_name}.csv"
        new_rows = []
        
        print(f"  Processing {split_name} set...")
        
        for row in rows:
            old_binary_path = row['path']
            old_metadata_path = row['metadata_path']
            filename = os.path.basename(old_binary_path)
            
            # ìƒˆ ê²½ë¡œ ì„¤ì •
            new_binary_path = os.path.join(base_binary_dir, split_name, filename)
            new_metadata_path = os.path.join(base_metadata_dir, split_name, filename + ".meta")
            
            # íŒŒì¼ ì´ë™ (ì‹¤ì œ ê²½ë¡œê°€ ë‹¤ë¥¼ ê²½ìš°ì—ë§Œ)
            # ì£¼ì˜: ìƒëŒ€ ê²½ë¡œ ì²˜ë¦¬ë¥¼ ìœ„í•´ ì ˆëŒ€ ê²½ë¡œë¡œ ë³€í™˜í•˜ì—¬ ë¹„êµí•˜ê±°ë‚˜, ë‹¨ìˆœíˆ ì´ë™ ì‹œë„
            try:
                if os.path.abspath(old_binary_path) != os.path.abspath(new_binary_path):
                    if os.path.exists(old_binary_path):
                        move_file(old_binary_path, new_binary_path)
                
                if os.path.abspath(old_metadata_path) != os.path.abspath(new_metadata_path):
                    if os.path.exists(old_metadata_path):
                        move_file(old_metadata_path, new_metadata_path)
            except Exception as e:
                print(f"    âŒ íŒŒì¼ ì´ë™ ì‹¤íŒ¨: {filename} - {e}")
                continue # ì´ë™ ì‹¤íŒ¨ ì‹œ í•´ë‹¹ í•­ëª© ì œì™¸? ì•„ë‹ˆë©´ ê²½ê³ ë§Œ? ì¼ë‹¨ ì§„í–‰

            # CSVìš© ê²½ë¡œ ì—…ë°ì´íŠ¸ (ìƒëŒ€ ê²½ë¡œ ìœ ì§€)
            new_row = row.copy()
            new_row['path'] = new_binary_path
            new_row['metadata_path'] = new_metadata_path
            new_rows.append(new_row)

        # CSV ì €ì¥
        if new_rows:
            with open(new_csv_path, 'w', newline='') as f:
                writer = csv.DictWriter(f, fieldnames=['path', 'metadata_path', 'target', 'class'])
                writer.writeheader()
                writer.writerows(new_rows)
            print(f"    âœ… {new_csv_path} ì €ì¥ ì™„ë£Œ")

    print(f"\n{'='*80}")
    print("âœ… ëª¨ë“  ì‘ì—… ì™„ë£Œ!")
    print(f"{'='*80}")

if __name__ == "__main__":
    parser = argparse.ArgumentParser(
        description="CSVì—ì„œ insn_addrì´ ë¹„ì–´ìˆëŠ” íŒŒì¼ ì œê±° ë° ë°ì´í„°ì…‹ ì¬êµ¬ì„±"
    )
    parser.add_argument(
        "--csv",
        type=str,
        action='append',
        required=True,
        help="ì…ë ¥ CSV íŒŒì¼ ê²½ë¡œ (ì—¬ëŸ¬ ê°œ ì§€ì • ê°€ëŠ¥)"
    )
    parser.add_argument(
        "--root-dir",
        type=str,
        default=None,
        help="ë©”íƒ€ë°ì´í„° íŒŒì¼ì˜ ë£¨íŠ¸ ë””ë ‰í† ë¦¬"
    )
    parser.add_argument(
        "--reorganize",
        action='store_true',
        help="ë°ì´í„°ì…‹ ì¬êµ¬ì„± (ì…”í”Œ, ë¶„í• , íŒŒì¼ ì´ë™) ìˆ˜í–‰"
    )
    
    args = parser.parse_args()
    
    if args.reorganize:
        reorganize_dataset(args.csv, args.root_dir)
    else:
        print("âš ï¸ --reorganize ì˜µì…˜ì„ ì‚¬ìš©í•´ì£¼ì„¸ìš”.")

